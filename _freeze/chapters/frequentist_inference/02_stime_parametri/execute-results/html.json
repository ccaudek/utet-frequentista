{
  "hash": "838e9ccd3750e0f5668f1bfb7c043f06",
  "result": {
    "engine": "knitr",
    "markdown": "# Stime, stimatori e parametri {#sec-freq-stime-parametri}\n\n\n::: callout-important\n## In questo capitolo imparerai a\n\n- Comprendere il concetto di distribuzione campionaria.\n- Familiarizzare con le proprietà della distribuzione campionaria della media dei campioni.\n- Comprendere il teorema del limite centrale.\n- Acquisire conoscenze sulle proprietà della distribuzione campionaria della varianza.\n:::\n\n::: callout-tip\n## Prerequisiti\n\n- Leggere il capitolo *Sampling* di [Statistical Inference via Data Science: A ModernDive into R and the Tidyverse (Second Edition)](https://moderndive.com/v2/).\n:::\n\n::: callout-caution\n## Preparazione del Notebook\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhere::here(\"code\", \"_common.R\") |> \n  source()\n```\n:::\n\n:::\n\n\n## Introduzione \n\nIn questo capitolo, ci concentreremo sul concetto di **distribuzione campionaria**, uno dei pilastri dell'inferenza statistica frequentista. La distribuzione campionaria descrive come le stime dei parametri della popolazione, come la media o la varianza, variano da campione a campione. Essa permette di stabilire proprietà probabilistiche delle stime campionarie, come la loro media e varianza, che sono fondamentali per costruire intervalli di confidenza e condurre test di ipotesi, strumenti essenziali dell'inferenza statistica frequentista.\n\n::: callout-tip\n## Domande introduttive\n\nPrima di iniziare a esplorare il concetto di distribuzione campionaria e il suo ruolo nell'inferenza statistica, considera le seguenti domande. Prova a formulare una risposta intuitiva basata sulle tue conoscenze attuali prima di procedere con la lettura del capitolo. Le simulazioni che seguiranno ti aiuteranno a confermare o rivedere le tue risposte.\n\n1. Se estraiamo ripetutamente campioni casuali dalla stessa popolazione e calcoliamo la loro media, come saranno distribuite queste medie campionarie rispetto alla media della popolazione?\n\n2. Supponiamo di estrarre campioni di dimensione $n=2$ da una popolazione. L'insieme delle medie campionarie sarà più concentrato intorno alla media della popolazione rispetto ai valori individuali della popolazione stessa?\n\n3. Quale relazione esiste tra la dimensione del campione $n$ e la variabilità delle medie campionarie? In altre parole, se aumentiamo la dimensione del campione, cosa succede alla dispersione della distribuzione campionaria?\n\n4. La distribuzione campionaria della media campionaria sarà sempre normale? Quali fattori influenzano la sua forma?\n\n5. La media della distribuzione campionaria coincide sempre con la media della popolazione? E la sua varianza è maggiore o minore rispetto alla varianza della popolazione?\n\n6. Supponiamo di estrarre piccoli campioni da una popolazione che ha una distribuzione fortemente asimmetrica. Quale forma avrà la distribuzione delle medie campionarie per piccoli campioni? E per campioni più grandi?\n:::\n\n## Stime, stimatori e parametri\n\nDopo aver esplorato il contesto culturale del frequentismo nel capitolo precedente, ci spostiamo ora su un piano strettamente statistico per introdurre il concetto di **stima statistica**.\n\nQuando si analizzano i dati, l’obiettivo è spesso quello di ottenere informazioni su una caratteristica della popolazione. Tuttavia, nella maggior parte dei casi, si ha accesso solo a un campione di osservazioni. La quantità sconosciuta che vogliamo stimare viene chiamata **parametro**, mentre il valore che calcoliamo dal campione per approssimare questo parametro è la **stima**. La formula o il procedimento matematico che utilizziamo per ottenere la stima è detto **stimatore**. Formalmente, uno stimatore è una funzione dei dati osservati utilizzata per produrre una stima di un parametro.\n\nIn altre parole, quando analizziamo un campione, cerchiamo di inferire proprietà della popolazione da cui il campione è tratto. Il parametro rappresenta una misura di queste proprietà, ma raramente può essere calcolato direttamente sulla popolazione intera. Pertanto, utilizziamo le osservazioni campionarie per ottenere una stima del parametro. La stima è quindi un’approssimazione del valore del parametro basata sui dati raccolti, mentre lo stimatore è la regola matematica o statistica che la produce.\n\nÈ importante sottolineare che le stime non coincidono necessariamente con il vero valore del parametro, poiché sono soggette a incertezza dovuta alla variabilità del campionamento. In questo capitolo esamineremo come l’approccio frequentista quantifica questa incertezza e come possiamo utilizzare tale quantificazione per trarre conclusioni affidabili sui parametri di interesse.\n\n## Distribuzione campionaria\n\nNell'inferenza frequentista applicata alla psicologia, il parametro di maggiore interesse è spesso la media della popolazione—si veda anche la discussione nella @sec-eda-ergodic-fallacy. In questo capitolo esploreremo come la media di un campione casuale possa essere utilizzata per stimare la media $\\mu$ di una popolazione. Per valutare l'incertezza associata a questa stima, introdurremo il concetto di **distribuzione campionaria**, un principio fondamentale dell'approccio frequentista.\n\nPer chiarire questa idea, inizieremo con un esempio basato su una popolazione finita di piccole dimensioni, pur consapevoli che le proprietà illustrate si estendono anche a popolazioni di dimensioni maggiori.\n\n### Esempio introduttivo\n\nConsideriamo la seguente popolazione:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nx <- c(2, 4.5, 5, 5.5)\nx\n#> [1] 2.0 4.5 5.0 5.5\n```\n:::\n\n\nQuesti valori potrebbero rappresentare il tempo di reazione (in secondi) di quattro partecipanti a un esperimento di decisione rapida, in cui devono identificare il colore di uno stimolo visivo. In questo caso, l'intera popolazione è costituita da tutti i partecipanti disponibili per lo studio, ad esempio se l’esperimento è stato condotto in un piccolo gruppo di persone selezionate per caratteristiche specifiche, come quattro gemelli monozigoti in uno studio sulle differenze cognitive intra-familiari.\n\nL'istogramma sottostante rappresenta la distribuzione di frequenza della popolazione:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Creazione del dataframe\ndf <- data.frame(Valori = c(2, 4.5, 5, 5.5))\n\n# Istogramma con ggplot2\nggplot(df, aes(x = Valori)) +\n  geom_histogram(bins = 5, aes(y = after_stat(density)), color = \"white\") +\n  labs(title = \"Distribuzione della popolazione\", x = \"Valori\", y = \"Densità\") \n```\n\n::: {.cell-output-display}\n![](02_stime_parametri_files/figure-html/unnamed-chunk-3-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\nCalcoliamo la media e la varianza della popolazione:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmean(x)  # Media\n#> [1] 4.25\nvar(x)   # Varianza\n#> [1] 2.42\n```\n:::\n\n\n### Campionamento  \n\nConsideriamo ora tutti i possibili campioni di dimensione $n = 2$ che possiamo estrarre dalla popolazione. Poiché ogni valore può essere selezionato indipendentemente in entrambe le posizioni del campione, il numero totale di combinazioni possibili si ottiene con il calcolo combinatorio:  \n\n$$\n\\text{Numero totale di campioni} = k^n ,\n$$\n\ndove $k$ è la dimensione della popolazione e $n$ è la dimensione del campione. Nel nostro caso, con $k = 4$ e $n = 2$, otteniamo:  \n\n$$\n4^2 = 16 .\n$$\n\nPossiamo generare esplicitamente queste combinazioni con il seguente codice:  \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsamples <- expand.grid(x, x)\nsamples\n#>    Var1 Var2\n#> 1   2.0  2.0\n#> 2   4.5  2.0\n#> 3   5.0  2.0\n#> 4   5.5  2.0\n#> 5   2.0  4.5\n#> 6   4.5  4.5\n#> 7   5.0  4.5\n#> 8   5.5  4.5\n#> 9   2.0  5.0\n#> 10  4.5  5.0\n#> 11  5.0  5.0\n#> 12  5.5  5.0\n#> 13  2.0  5.5\n#> 14  4.5  5.5\n#> 15  5.0  5.5\n#> 16  5.5  5.5\n```\n:::\n\n\nOgni riga rappresenta un campione possibile. Confermiamo il numero totale di campioni con:  \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nnrow(samples)\n#> [1] 16\n```\n:::\n\n\nOra calcoliamo la media di ciascun campione, ottenendo la **distribuzione campionaria delle medie** per $n = 2$:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsample_means <- rowMeans(samples)\nsample_means\n#>  [1] 2.00 3.25 3.50 3.75 3.25 4.50 4.75 5.00 3.50 4.75 5.00 5.25 3.75 5.00 5.25\n#> [16] 5.50\n```\n:::\n\n\nQuesta distribuzione campionaria mostra tutte le possibili medie che possiamo ottenere estraendo campioni casuali di dimensione 2 dalla popolazione. Si tratta di un concetto fondamentale in inferenza statistica frequentista, poiché la distribuzione delle medie campionarie diventa progressivamente più simmetrica e concentrata attorno alla media della popolazione man mano che $n$ aumenta, come previsto dal **teorema del limite centrale**.\n\n### Visualizzazione della distribuzione campionaria\n\nPossiamo visualizzare la distribuzione campionaria delle medie con un istogramma:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Creazione del dataframe\ndf <- data.frame(Valori = sample_means)\n\n# Istogramma con ggplot2\nggplot(df, aes(x = Valori)) +\n  geom_histogram(bins = 5, aes(y = after_stat(density)), color = \"white\") +\n  labs(x = \"Media campionaria\", y = \"Densità\") \n```\n\n::: {.cell-output-display}\n![](02_stime_parametri_files/figure-html/unnamed-chunk-8-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\nL'istogramma mostra come le medie campionarie non siano distribuite uniformemente, ma seguano una struttura precisa determinata dalla distribuzione dei valori originali della popolazione. Con un numero maggiore di osservazioni per campione ($n$ più grande), la distribuzione campionaria delle medie tende a diventare più stretta e simmetrica attorno alla media della popolazione, illustrando così il principio alla base dell'inferenza statistica frequentista.\n\n### Verifiche teoriche  \n\n#### Media della distribuzione campionaria  \n\nSecondo la teoria statistica, la media della distribuzione campionaria deve coincidere con la media della popolazione. Questo implica che, se prendiamo la media di tutti i campioni possibili di una certa dimensione, il valore risultante sarà uguale alla media della popolazione stessa. Possiamo verificarlo con il seguente calcolo:  \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmean(x)  # Media della popolazione\n#> [1] 4.25\nmean(sample_means)  # Media della distribuzione campionaria\n#> [1] 4.25\n```\n:::\n\n\n#### Varianza della distribuzione campionaria  \n\nUn altro risultato importante è che la varianza della distribuzione campionaria delle medie è inferiore alla varianza della popolazione. In particolare, la teoria prevede che sia pari alla varianza della popolazione divisa per la dimensione del campione $n$:  \n\n$$\n\\mathbb{V}(\\bar{X}) = \\frac{\\sigma^2}{n}\n$$\n\nPoiché in questo caso $n = 2$, confrontiamo la varianza teorica con quella empirica:  \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Funzione per calcolare la varianza senza la correzione di Bessel\nvariance <- function(x) {\n  mean((x - mean(x))^2)\n}\n```\n:::\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nvariance(x) / 2  # Varianza teorica\n#> [1] 0.906\nvariance(sample_means)  # Varianza empirica\n#> [1] 0.906\n```\n:::\n\n\nOsserviamo che la varianza delle medie campionarie è inferiore alla varianza della popolazione, confermando che le medie campionarie mostrano meno variabilità rispetto alle singole osservazioni.  \n\n### Esempio di campione osservato  \n\nPer comprendere meglio questi concetti, consideriamo un singolo campione, ad esempio:  \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nobserved_sample <- c(5, 5.5)\nobserved_sample\n#> [1] 5.0 5.5\n```\n:::\n\n\nCalcoliamo la sua media e deviazione standard:  \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmean(observed_sample)  # Media del campione\n#> [1] 5.25\nsqrt(variance(observed_sample))  # Deviazione standard del campione\n#> [1] 0.25\n```\n:::\n\n\nOra confrontiamo questi valori con quelli della popolazione:  \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmean(x)  # Media della popolazione\n#> [1] 4.25\nsqrt(variance(x))  # Deviazione standard della popolazione\n#> [1] 1.35\n```\n:::\n\n\nOsserviamo che la media del campione si avvicina a quella della popolazione, ma non coincide necessariamente con essa. Questo è del tutto normale: ogni campione rappresenta solo una porzione della popolazione e la sua media può variare leggermente a seconda delle osservazioni selezionate.  \n\nPer quanto riguarda la deviazione standard, in questo caso specifico risulta inferiore a quella della popolazione. Tuttavia, in generale, la dispersione di un singolo campione può essere maggiore o minore rispetto a quella della popolazione, poiché dipende dalla variabilità casuale delle osservazioni estratte. Proprio per questo motivo, per trarre inferenze affidabili sulla popolazione, è più utile considerare la distribuzione campionaria delle medie piuttosto che un singolo campione isolato.  \n\nQuesto esempio illustra bene il principio della stima campionaria: mentre un singolo campione fornisce un'informazione parziale, l'analisi di molteplici campioni consente di ottenere una stima più precisa e stabile della media della popolazione, riducendo l’incertezza e migliorando l'affidabilità dell’inferenza statistica.\n\n### La Simulazione Illustra Due Principi  \n\nDalla simulazione emergono due principi fondamentali dell'inferenza statistica:  \n\n1. **La media della distribuzione campionaria** coincide con la media della popolazione. Questo implica che se estraiamo molteplici campioni di dimensione $n$ e calcoliamo la loro media, il valore atteso della media campionaria sarà uguale alla media della popolazione $\\mu$. Formalmente:  \n\n   $$\n   \\mathbb{E}(\\bar{X}_n) = \\mu .\n   $$\n\n   Questo risultato conferma che la media campionaria è uno stimatore non distorto della media della popolazione.  \n\n2. **La varianza della distribuzione campionaria** è minore della varianza della popolazione. Questo riflette il fatto che le medie campionarie tendono a essere più stabili rispetto alle singole osservazioni. La relazione teorica che descrive questa proprietà è:  \n\n   $$\n   \\mathbb{V}(\\bar{X}) = \\frac{\\sigma^2}{n} .\n   $$\n\n   Ciò significa che, aumentando la dimensione del campione $n$, la variabilità delle medie campionarie si riduce, rendendo la stima della media della popolazione più precisa. Questo concetto è alla base della teoria del **teorema centrale del limite**, che diventa sempre più evidente con campioni di dimensioni maggiori.\n   \n::: {.callout-note title=\"Dimostrazione che $\\bar{X}$ è uno stimatore corretto della media della popolazione\" collapse=\"true\"}\n\nDato un campione casuale di $n$ osservazioni $X_1, X_2, \\dots, X_n$ estratte da una popolazione con media $\\mu$ e varianza $\\sigma^2$, la **media campionaria** è definita come:\n\n$$\n\\bar{X}_n = \\frac{1}{n} \\sum_{i=1}^{n} X_i .\n$$\n\nUtilizziamo la linearità dell'operatore di aspettativa:\n\n$$\n\\mathbb{E}(\\bar{X}_n) = \\mathbb{E} \\left( \\frac{1}{n} \\sum_{i=1}^{n} X_i \\right) .\n$$\n\nPer la proprietà della linearità dell'aspettativa, possiamo portare fuori il fattore costante $\\frac{1}{n}$:\n\n$$\n\\mathbb{E}(\\bar{X}_n) = \\frac{1}{n} \\sum_{i=1}^{n} \\mathbb{E}(X_i) .\n$$\n\nPoiché ogni $X_i$ proviene dalla stessa popolazione, ha la stessa aspettativa $\\mathbb{E}(X_i) = \\mu$, quindi:\n\n$$\n\\mathbb{E}(\\bar{X}_n) = \\frac{1}{n} \\sum_{i=1}^{n} \\mu .\n$$\n\nSommando $n$ volte $\\mu$, otteniamo:\n\n$$\n\\mathbb{E}(\\bar{X}_n) = \\frac{1}{n} (n \\mu) = \\mu .\n$$\n\nIn conclusione, abbiamo dimostrato che:\n\n$$\n\\mathbb{E}(\\bar{X}_n) = \\mu .\n$$\n\nQuesto significa che la media campionaria $\\bar{X}_n$ è uno **stimatore corretto** (non distorto) della media della popolazione $\\mu$, poiché il suo valore atteso coincide esattamente con la quantità che vogliamo stimare.\n:::\n\n::: {.callout-note title=\"Dimostrazione della riduzione della varianza nelle medie campionarie\" collapse=\"true\"}\n\nSia $X_1, X_2, \\dots, X_n$ un campione casuale di $n$ osservazioni indipendenti estratte da una popolazione con media $\\mu$ e varianza $\\sigma^2$. La media campionaria è definita come:\n\n$$\n\\bar{X} = \\frac{1}{n} \\sum_{i=1}^n X_i.\n$$\n  \nPer definizione, la varianza di $\\bar{X}$ è:\n\n$$\n\\mathbb{V}(\\bar{X}) = \\mathbb{V}\\left(\\frac{1}{n} \\sum_{i=1}^n X_i\\right).\n$$\n  \nPoiché una costante moltiplicata da una variabile aleatoria può essere \"estratta\" dalla varianza, otteniamo:\n\n$$\n\\mathbb{V}(\\bar{X}) = \\frac{1}{n^2} \\mathbb{V}\\left(\\sum_{i=1}^n X_i\\right).\n$$\n  \nOra dobbiamo calcolare $\\mathbb{V}\\left(\\sum_{i=1}^n X_i\\right)$. Le variabili $X_1, X_2, \\dots, X_n$ sono indipendenti, quindi la varianza della somma è la somma delle varianze:\n\n$$\n\\mathbb{V}\\left(\\sum_{i=1}^n X_i\\right) = \\sum_{i=1}^n \\mathbb{V}(X_i).\n$$\n  \nPoiché tutte le variabili $X_i$ hanno la stessa varianza $\\sigma^2$:\n\n$$\n\\mathbb{V}\\left(\\sum_{i=1}^n X_i\\right) = n \\sigma^2.\n$$\n  \nSostituendo nella formula precedente, otteniamo:\n\n$$\n\\mathbb{V}(\\bar{X}) = \\frac{1}{n^2} \\cdot n \\sigma^2 = \\frac{\\sigma^2}{n}.\n$$\n\nIn generale, dunque, per un campione di ampiezza $n$, vale la relazione $\\mathbb{V}(\\bar{X}) = \\frac{\\sigma^2}{n}$.\n   \n:::\n\n## Proprietà della distribuzione campionaria  \n\nUna caratteristica fondamentale della distribuzione campionaria riguarda la sua forma e il modo in cui dipende dalla distribuzione della popolazione da cui vengono estratti i campioni. Possiamo distinguere due casi principali:  \n\n- **Se la popolazione segue una distribuzione normale**, allora anche la distribuzione delle medie campionarie sarà normalmente distribuita, indipendentemente dalla dimensione del campione $n$. Questo significa che, anche con campioni molto piccoli, la media campionaria manterrà la stessa forma della distribuzione originale.  \n\n- **Se la popolazione non segue una distribuzione normale**, entra in gioco il **teorema centrale del limite**. Questo teorema afferma che, man mano che la dimensione del campione $n$ aumenta, la distribuzione delle medie campionarie tenderà comunque a una distribuzione normale, indipendentemente dalla forma della distribuzione di partenza. In pratica, per campioni sufficientemente grandi, possiamo approssimare la distribuzione delle medie campionarie con una normale, anche se la popolazione da cui provengono i dati è asimmetrica o non gaussiana.  \n\nQueste proprietà sono fondamentali nell'inferenza statistica frequentista: permettono di stimare e testare parametri della popolazione utilizzando campioni, facilitando l'applicazione di strumenti basati sulla distribuzione normale, come gli intervalli di confidenza e i test di ipotesi.\n\n## Teorema del Limite Centrale\n\nEsaminiamo ora più in dettaglio il **Teorema del Limite Centrale (TLC)**. Nel 1812, Pierre-Simon Laplace dimostrò il TLC, che afferma che la somma (o la media) di una sequenza di variabili casuali indipendenti e identicamente distribuite (i.i.d.) tende a distribuirsi secondo una distribuzione Normale (o Gaussiana), al crescere della dimensione del campione. Inoltre, il TLC specifica i parametri della distribuzione Normale risultante in base ai valori attesi e alle varianze delle variabili casuali sommate.\n\n::: {#thm-}\nSi consideri una sequenza di variabili aleatorie indipendenti e identicamente distribuite (i.i.d.) $Y_1, Y_2, \\dots, Y_n$, con valore atteso $\\mathbb{E}(Y_i) = \\mu$ e deviazione standard $\\text{SD}(Y_i) = \\sigma.$ Si definisca una nuova variabile casuale come la media campionaria:\n\n$$\nZ = \\frac{1}{n} \\sum_{i=1}^n Y_i.\n$$\n\nAl tendere di $n$ all'infinito ($n \\rightarrow \\infty$), la distribuzione di $Z$ converge a una distribuzione Normale con valore atteso $\\mu$ e deviazione standard $\\frac{\\sigma}{\\sqrt{n}}$:\n\n$$\nZ \\sim \\mathcal{N}\\left(\\mu, \\, \\frac{\\sigma}{\\sqrt{n}} \\right).\n$$\n\nIn altre parole, la densità di probabilità di $Z$ tende a:\n\n$$\np_Z(z) \\rightarrow \\mathcal{N}\\left(z \\ \\Bigg| \\ \\mu, \\, \\frac{\\sigma}{\\sqrt{n}} \\right).\n$$\n:::\n\nIl TLC può essere generalizzato anche a variabili casuali che non sono identicamente distribuite, purché siano indipendenti e abbiano valori attesi e varianze finite. Questo teorema spiega perché molti fenomeni naturali, come l'altezza degli adulti o il peso di una popolazione, tendono a seguire una distribuzione Normale. Infatti, tali fenomeni sono spesso il risultato di una combinazione di numerosi effetti additivi e indipendenti, ciascuno dei quali contribuisce in modo relativamente piccolo. Indipendentemente dalla distribuzione individuale di ciascun effetto, la loro somma (o media) tende a distribuirsi in modo Normale. Questa è la ragione per cui la distribuzione Normale fornisce una buona approssimazione per la distribuzione di molti fenomeni osservati in natura.\n\n### Illustrazione del Teorema del Limite Centrale (TLC)  \n\nPer comprendere il **Teorema del Limite Centrale (TLC)**, consideriamo una popolazione iniziale che segue una distribuzione fortemente asimmetrica: la distribuzione **Beta(2,1)**, caratterizzata da una forte asimmetria positiva.  \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Parametri della distribuzione Beta\na <- 2\nb <- 1\n\n# Genera valori per la distribuzione Beta\nx <- seq(0, 1, length.out = 1000)  # Valori tra 0 e 1\ny <- dbeta(x, shape1 = a, shape2 = b)  # Densità della distribuzione Beta\n\n# Crea un dataframe per qplot\ndata <- data.frame(x = x, y = y)\n\n# Grafico con qplot\nqplot(x, y, data = data, geom = \"line\", \n      main = \"Distribuzione Beta(2, 1)\", \n      xlab = \"x\", \n      ylab = \"Densità\")\n```\n\n::: {.cell-output-display}\n![](02_stime_parametri_files/figure-html/unnamed-chunk-15-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\nEstrarremo più volte campioni casuali di ampiezza $n$ da questa popolazione e calcoleremo le medie campionarie. Il TLC prevede che, all'aumentare della dimensione del campione, la distribuzione delle medie campionarie tenda a una distribuzione normale, indipendentemente dalla forma della popolazione di partenza.  \n\nPer verificare questa proprietà, definiamo una funzione che genera campioni, calcola le medie e visualizza la loro distribuzione campionaria per diversi valori di $n$:  \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Parametri della distribuzione Beta\nalpha <- 2\nbeta <- 1\n\n# Funzione per simulare e visualizzare la distribuzione campionaria\nplot_samples <- function(n) {\n  # Media e deviazione standard della distribuzione Beta\n  mu <- alpha / (alpha + beta)\n  sigma <- sqrt(alpha * beta / ((alpha + beta)^2 * (alpha + beta + 1)))\n  \n  # Generazione di 50.000 campioni casuali di dimensione n\n  sample_means <- replicate(50000, mean(rbeta(n, alpha, beta)))\n  \n  # Creazione del dataframe\n  df <- data.frame(MediaCampionaria = sample_means)\n  \n  # Creazione del grafico con ggplot2\n  ggplot(df, aes(x = MediaCampionaria)) +\n    geom_histogram(aes(y = after_stat(density)), bins = 50, color = \"white\") +\n    stat_function(fun = dnorm, args = list(mean = mu, sd = sigma / sqrt(n)), color = \"black\", lwd = 1.2) +\n    labs(title = paste(\"Distribuzione campionaria\\nper n =\", n),\n         x = \"Media campionaria\",\n         y = \"Densità\") \n}\n```\n:::\n\n\n\n#### Visualizzazione della convergenza alla normalità  \n\nAnalizziamo l'effetto della dimensione del campione sulle medie campionarie:  \n\n1. **Campioni di ampiezza $n = 1$**  \n\n   Se $n = 1$, la distribuzione campionaria coincide esattamente con la distribuzione della popolazione di partenza, che in questo caso è fortemente asimmetrica:  \n\n\n   ::: {.cell layout-align=\"center\"}\n   \n   ```{.r .cell-code}\n   plot_samples(1)\n   ```\n   \n   ::: {.cell-output-display}\n   ![](02_stime_parametri_files/figure-html/unnamed-chunk-17-1.png){fig-align='center' width=85%}\n   :::\n   :::\n\n\n2. **Campioni di ampiezza $n = 2$**  \n\n   Con $n = 2$, la distribuzione delle medie campionarie inizia a perdere parte della sua asimmetria:  \n\n\n   ::: {.cell layout-align=\"center\"}\n   \n   ```{.r .cell-code}\n   plot_samples(2)\n   ```\n   \n   ::: {.cell-output-display}\n   ![](02_stime_parametri_files/figure-html/unnamed-chunk-18-1.png){fig-align='center' width=85%}\n   :::\n   :::\n\n\n3. **Campioni di ampiezza $n = 4$**  \n\n   Per $n = 4$, la distribuzione delle medie campionarie diventa più simmetrica e tende già a una forma più vicina a quella normale:  \n\n\n   ::: {.cell layout-align=\"center\"}\n   \n   ```{.r .cell-code}\n   plot_samples(4)\n   ```\n   \n   ::: {.cell-output-display}\n   ![](02_stime_parametri_files/figure-html/unnamed-chunk-19-1.png){fig-align='center' width=85%}\n   :::\n   :::\n\n\n4. **Campioni di ampiezza $n = 30$**  \n\n   Quando $n$ diventa sufficientemente grande (ad esempio $n = 30$), la distribuzione campionaria delle medie è praticamente indistinguibile da una normale:  \n\n\n   ::: {.cell layout-align=\"center\"}\n   \n   ```{.r .cell-code}\n   plot_samples(30)\n   ```\n   \n   ::: {.cell-output-display}\n   ![](02_stime_parametri_files/figure-html/unnamed-chunk-20-1.png){fig-align='center' width=85%}\n   :::\n   :::\n\n\n\n#### Conclusione  \n\nIl **Teorema del Limite Centrale (TLC)** afferma che, indipendentemente dalla forma della distribuzione della popolazione:  \n\n- Se la dimensione del campione è sufficientemente grande, la distribuzione delle medie campionarie $\\bar{X}$ sarà **approssimativamente normale**, anche se la popolazione di partenza non lo è.  \n- La distribuzione delle medie campionarie avrà media uguale a quella della popolazione $\\mu$ e deviazione standard pari a:  \n\n  $$\n  \\bar{X} \\sim \\mathcal{N}(\\mu, \\sigma / \\sqrt{n})\n  $$\n\n  dove $\\sigma$ è la deviazione standard della popolazione e $n$ è la dimensione del campione.  \n\n### Implicazioni  \n\n1. **Normalità emergente**  \n   Il TLC giustifica l'uso della distribuzione normale in molte applicazioni statistiche, anche quando i dati originali non seguono una distribuzione normale.  \n\n2. **Errore standard e precisione delle stime**  \n   Il TLC fornisce una formula esplicita per calcolare l'**errore standard** $\\sigma / \\sqrt{n}$, che quantifica l’incertezza associata alla media campionaria. All’aumentare di $n$, l’errore standard diminuisce, migliorando la precisione della stima della media della popolazione.  \n\nQuesta proprietà è alla base di molte tecniche statistiche, come gli intervalli di confidenza e i test di ipotesi, che assumono la normalità della distribuzione campionaria delle medie anche quando la popolazione di partenza non è normale.\n\n\n### Applicazioni in psicologia\n\nMolti fenomeni psicologici che misuriamo (ad esempio, il QI come media di molte abilità cognitive) derivano dalla media di più variabili, e quindi seguono la distribuzione normale grazie al TLC. Questo spiega perché la distribuzione normale appare così frequentemente nei dati sperimentali di psicologia e in molte altre discipline scientifiche.\n\n## Distribuzioni campionarie di altre statistiche\n\nAbbiamo già analizzato la distribuzione campionaria della media dei campioni. Tuttavia, è possibile costruire distribuzioni campionarie per altre statistiche campionarie. Ad esempio, consideriamo la distribuzione campionaria del valore massimo e della varianza.\n\n### Distribuzione campionaria del valore massimo\n\nSupponiamo di avere una popolazione normalmente distribuita con media $\\mu = 100$ e deviazione standard $\\sigma = 15$. Generiamo 10.000 campioni casuali di ampiezza $n = 5$ e calcoliamo il valore massimo per ogni campione. \n\n#### Simulazione e visualizzazione\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(123)  # Per risultati riproducibili\n\n# Parametri della distribuzione\nmu <- 100\nsigma <- 15\n\n# Simulazione: calcolo del valore massimo per ciascun campione\nn_samples <- 10000\nsample_maxes <- replicate(\n  n_samples, \n  max(rnorm(5, mean = mu, sd = sigma))\n)\n\n# Creazione del dataframe\ndf <- data.frame(ValoreMassimo = sample_maxes)\n\n# Istogramma con ggplot2\nggplot(df, aes(x = ValoreMassimo)) +\n  geom_histogram(aes(y = after_stat(density)), bins = 50, color = \"white\") +\n  stat_function(fun = dnorm, args = list(mean = mu, sd = sigma), color = \"black\", lwd = 1.2) +\n  labs(x = \"Valore massimo\",\n       y = \"Densità\")\n```\n\n::: {.cell-output-display}\n![](02_stime_parametri_files/figure-html/unnamed-chunk-21-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\nOsserviamo che il valore atteso della distribuzione campionaria del massimo è maggiore della media della popolazione $\\mu$.\n\n### Distribuzione campionaria della varianza\n\nUn'altra statistica interessante è la varianza campionaria. La formula della varianza campionaria, basata sulla statistica descrittiva, è:\n\n$$\nS^2 = \\frac{\\sum_{i=1}^n (Y_i - \\bar{Y})^2}{n}.\n$$\n\nCalcoliamo la distribuzione campionaria della varianza per campioni di ampiezza $n = 5$.\n\n#### Simulazione e visualizzazione\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(123)\n\n# Parametri della distribuzione\nmu <- 100\nsigma <- 15\nn_samples <- 10000\n\n# Funzione per calcolare la varianza senza la correzione di Bessel\nvariance <- function(x) {\n  mean((x - mean(x))^2)  # Divisione per n invece di (n-1)\n}\n\n# Simulazione: calcolo della varianza per ciascun campione\nsample_vars <- replicate(\n  n_samples, \n  variance(rnorm(5, mean = mu, sd = sigma))\n)\n\n# Creazione del dataframe\ndf <- data.frame(Varianza = sample_vars)\n\n# Istogramma con ggplot2\nggplot(df, aes(x = Varianza)) +\n  geom_histogram(aes(y = after_stat(density)), bins = 50, color = \"white\") +\n  labs(x = \"Varianza\",\n       y = \"Densità\")\n```\n\n::: {.cell-output-display}\n![](02_stime_parametri_files/figure-html/unnamed-chunk-22-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Media empirica della varianza campionaria\nmean(sample_vars)\n#> [1] 181\n```\n:::\n\n\nSappiamo che la varianza della popolazione è $\\sigma^2 = 15^2 = 225$. Tuttavia, il valore medio empirico delle varianze campionarie calcolate con $S^2$ risulta minore di 225. Questo avviene perché lo stimatore $S^2$ è distorto.\n\n### Correzione della distorsione\n\nPer eliminare la distorsione, utilizziamo il seguente stimatore della varianza della popolazione:\n\n$$\ns^2 = \\frac{\\sum_{i=1}^n (Y_i - \\bar{Y})^2}{n-1}.\n$$\n\n#### Verifica con simulazione\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(123)\n\n# Simulazione: calcolo della varianza con la correzione\nsample_vars_unbiased <- replicate(\n  n_samples, \n  var(rnorm(5, mean = mu, sd = sigma))\n)\n\n# Creazione del dataframe\ndf <- data.frame(Varianza = sample_vars_unbiased)\n\n# Istogramma con ggplot2\nggplot(df, aes(x = Varianza)) +\n  geom_histogram(aes(y = after_stat(density)), bins = 50, color = \"white\") +\n  labs(x = \"Varianza\",\n       y = \"Densità\")\n```\n\n::: {.cell-output-display}\n![](02_stime_parametri_files/figure-html/unnamed-chunk-24-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Media empirica della varianza corretta\nmean(sample_vars_unbiased)\n#> [1] 226\n```\n:::\n\n\nCon questo stimatore, la media della distribuzione campionaria coincide con la varianza reale della popolazione $\\sigma^2 = 225$.\n\nIn conclusione:\n\n1. La **distribuzione campionaria del massimo** mostra che il valore massimo dei campioni è, in media, maggiore della media della popolazione.\n2. La **varianza campionaria non corretta** ($S^2$) è uno stimatore distorto, poiché il suo valore atteso non coincide con la varianza della popolazione.\n3. Lo stimatore corretto $s^2$, che utilizza il divisore $n - 1$, elimina la distorsione e fornisce una stima non distorta della varianza della popolazione.\n\nIn generale, uno stimatore è considerato **non distorto** quando il valore atteso delle sue stime coincide con il valore reale del parametro. Nel caso della media campionaria e della varianza corretta, entrambi gli stimatori sono non distorti.\n\n## Riflessioni Conclusive\n\nIn generale, i parametri della popolazione sono sconosciuti, ma possiamo stimarli utilizzando le informazioni del campione. Di seguito viene presentata una tabella che riassume i simboli comuni utilizzati per indicare le quantità note e sconosciute nel contesto dell'inferenza statistica. Questo ci aiuterà a tenere traccia di ciò che sappiamo e ciò che non sappiamo.\n\n|Simbolo          | Nome           | È qualcosa che conosciamo?     |\n|:----------------|:-------------|:--------------------|\n|$s$              |Deviazione standard del campione    |Sì, la calcoliamo dai dati grezzi |\n|$\\sigma$         |Deviazione standard della popolazione  | No, tranne in casi particolari o nelle simulazioni  |\n|$\\hat{\\sigma}$  | Stima della deviazione standard della popolazione | Sì, ma non è uguale a $\\sigma$ |\n|$s^2$            | Varianza del campione    |Sì, la calcoliamo dai dati grezzi |\n|$\\sigma^2$       | Varianza della popolazione  | No, tranne in casi particolari o nelle simulazioni  |\n|$\\hat{\\sigma}^2$ | Stima della varianza della popolazione  | Sì, ma non è uguale a $\\sigma^2$  |\n: {tbl-colwidths=\"[10, 40, 50]\"}\n\n\\\nUtilizzando le informazioni di un campione casuale di ampiezza $n$:\n\n- La stima migliore che possiamo ottenere per la media $\\mu$ della popolazione è la media del campione $\\bar{Y}$.\n- La stima migliore che possiamo ottenere per la varianza $\\sigma^2$ della popolazione è:\n\n$$\n\\hat{\\sigma}^2 = \\frac{1}{n-1} \\sum_{i=1}^n (Y_i - \\bar{Y})^2.\n$$\n\n::: {.callout-tip title=\"Risposte alle domande iniziali\" collapse=\"true\"}\n\nDopo aver esplorato il concetto di distribuzione campionaria attraverso simulazioni ed esempi pratici, possiamo ora confrontare le nostre risposte intuitive con quanto appreso:\n\n1. **Le medie campionarie tendono a distribuirsi intorno alla media della popolazione**, con una variabilità che dipende dalla dimensione del campione.\n\n2. **Sì, le medie campionarie sono meno disperse rispetto ai singoli valori della popolazione**, il che significa che forniscono una stima più stabile della media della popolazione.\n\n3. **All’aumentare di $n$, la distribuzione campionaria delle medie diventa più stretta**, ossia la variabilità delle medie campionarie si riduce. La varianza della distribuzione campionaria è pari a $\\sigma^2 / n$, dove $\\sigma^2$ è la varianza della popolazione.\n\n4. **No, la distribuzione campionaria della media è normale solo se la popolazione di partenza è normale o se la dimensione del campione è sufficientemente grande** (Teorema del Limite Centrale).\n\n5. **Sì, la media della distribuzione campionaria coincide con la media della popolazione. Tuttavia, la varianza della distribuzione campionaria è inferiore alla varianza della popolazione**, poiché viene divisa per la dimensione del campione ($n$).\n\n6. **Per campioni piccoli, la distribuzione delle medie campionarie somiglierà alla distribuzione della popolazione originale**. Se la popolazione è fortemente asimmetrica, anche la distribuzione campionaria per piccoli campioni sarà asimmetrica. Tuttavia, aumentando $n$, la distribuzione delle medie campionarie tenderà a una normale, indipendentemente dalla forma della popolazione di partenza.\n:::\n\n## Esercizi {.unnumbered}\n\n::: {.callout-important title=\"Problemi 1\" collapse=\"true\"}\n**Parte 1: Popolazione di Piccole Dimensioni**\nSi consideri una popolazione con i seguenti valori:\n\n$$\nx = \\{2, 4.5, 5, 5.5\\}\n$$\n\n1. Calcolare la media e la varianza della popolazione.\n2. Estrarre tutti i possibili campioni di ampiezza $n = 2$ con ripetizione e calcolare la media di ciascun campione.\n3. Rappresentare graficamente la distribuzione campionaria delle medie.\n4. Calcolare la probabilità che la media campionaria sia inferiore a 3:\n   - Esattamente, utilizzando la distribuzione campionaria.\n   - Approssimativamente, assumendo una distribuzione normale se il campione fosse sufficientemente grande.\n\n**Parte 2: Popolazione di Grandi Dimensioni**\n\nSi consideri ora una popolazione più grande, generata da una distribuzione normale con media $\\mu = 10$ e deviazione standard $\\sigma = 3$.\n\n1. Generare una popolazione di 1000 osservazioni.\n2. Calcolare la media e la varianza della popolazione.\n3. Estrarre 10.000 campioni casuali di ampiezza $n = 15$ e calcolare la media campionaria per ciascun campione.\n4. Rappresentare graficamente la distribuzione campionaria delle medie.\n5. Calcolare la probabilità che la media campionaria sia inferiore a 9:\n   - Esattamente, utilizzando la distribuzione campionaria.\n   - Approssimativamente, utilizzando la distribuzione normale.\n\n**Obiettivo**: Verificare sperimentalmente le proprietà della distribuzione campionaria della media e confrontare i risultati con le previsioni teoriche fornite dal Teorema del Limite Centrale.\n:::\n\n::: {.callout-tip title=\"Soluzioni 1\" collapse=\"true\"}\nProprietà della Distribuzione Campionaria della Media\n\n1. Media: La media della distribuzione campionaria coincide con la media della popolazione:\n\n2. Varianza: La varianza della distribuzione campionaria è pari alla varianza della popolazione divisa per la dimensione del campione:\n\n3. Forma:\n\n- Se la popolazione segue una distribuzione normale, anche la distribuzione campionaria della media sarà normale.\n- Se la popolazione non è normale, il Teorema del Limite Centrale garantisce che la distribuzione campionaria della media sarà approssimativamente normale per campioni di dimensioni sufficientemente grandi ($n \\geq 30$).\n\nDefiniamo una popolazione e calcoliamo i parametri:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Popolazione\nx <- c(2, 4.5, 5, 5.5)\nmean(x)  # Media della popolazione\n#> [1] 4.25\nvar(x)   # Varianza della popolazione\n#> [1] 2.42\n```\n:::\n\n\nEstrarre tutti i possibili campioni di ampiezza $n = 2$ e calcolare la media di ciascun campione:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Tutti i campioni di ampiezza 2\nsamples <- expand.grid(x, x)\nsample_means <- rowMeans(samples)\n\n# Visualizzare la distribuzione campionaria\ndf <- data.frame(sample_means = sample_means)\n\nggplot(df, aes(x = sample_means)) +\n  geom_histogram(aes(y = after_stat(density)), bins = 5, alpha = 0.6) +\n  geom_density(color = \"black\", size = 1) +\n  labs(\n    title = \"Distribuzione campionaria delle medie\",\n    x = \"Media campionaria\",\n    y = \"Densità\"\n  )\n```\n\n::: {.cell-output-display}\n![](02_stime_parametri_files/figure-html/unnamed-chunk-27-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\nCalcoliamo la probabilità che, all'interno della distribuzione campionaria, la media del campione sia minore di 3. Troviamo il valore esatto nella simulazione. Approssimiamo il valore esatto con il valore atteso se il campione fosse sufficientemente grande da poter assumere una distribuzione campionaria normale:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Probabilità esatta dalla distribuzione campionaria\nexact_probability <- mean(sample_means < 3)\nexact_probability\n#> [1] 0.0625\n\n# Approssimazione tramite distribuzione normale\nmu <- mean(x)  # Media della popolazione\nsigma <- sqrt(var(x) / 2)  # Deviazione standard della distribuzione campionaria\napprox_probability <- pnorm(3, mean = mu, sd = sigma)\napprox_probability\n#> [1] 0.128\n```\n:::\n\n\nRipetiamo ora l'esempio, mantenendo la stessa struttura della simulazione, ma considerando una popolazione più grande tale per cui si possa estrarre un campione di ampiezza 15:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Nuova popolazione\nset.seed(123)\nx_large <- rnorm(1000, mean = 10, sd = 3)  # Popolazione più grande\nmean(x_large)  # Media della popolazione\n#> [1] 10\nvar(x_large)   # Varianza della popolazione\n#> [1] 8.85\n\n# Estrazione di campioni di ampiezza 15\nsamples_large <- replicate(10000, mean(sample(x_large, size = 15)))\n\n# Creazione del data frame per ggplot2\ndf_large <- data.frame(sample_means = samples_large)\n\n# Visualizzazione con ggplot2\nggplot(df_large, aes(x = sample_means)) +\n  geom_histogram(aes(y = after_stat(density)), bins = 50, alpha = 0.6) +\n  geom_density(color = \"black\", size = 1) +\n  labs(\n    title = \"Distribuzione campionaria delle medie (n = 15)\",\n    x = \"Media campionaria\",\n    y = \"Densità\"\n  )\n```\n\n::: {.cell-output-display}\n![](02_stime_parametri_files/figure-html/unnamed-chunk-29-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Probabilità esatta dalla simulazione\nexact_probability_large <- mean(samples_large < 9)\nexact_probability_large\n#> [1] 0.0834\n```\n:::\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Approssimazione tramite distribuzione normale\nmu_large <- mean(x_large)\nsigma_large <- sqrt(var(x_large) / 15)\napprox_probability_large <- pnorm(9, mean = mu_large, sd = sigma_large)\napprox_probability_large\n#> [1] 0.0862\n```\n:::\n\n:::\n\n\n::: {.callout-important title=\"Problemi 2\" collapse=\"true\"}\n**Distribuzione Campionaria della Differenza tra Medie**\n\nL’obiettivo di questo esercizio è esplorare le proprietà della distribuzione campionaria della differenza tra medie campionarie, analizzando sia il caso di popolazioni finite di piccole dimensioni sia il caso di popolazioni più grandi, con distribuzioni normali.\n\n**Esercizio 1: Simulazione con Popolazioni di Piccole Dimensioni**\n\nConsideriamo due popolazioni finite composte da un numero limitato di elementi:\n\n- **Popolazione 1:** $x_1 = \\{2, 4.5, 5, 6\\}$\n- **Popolazione 2:** $x_2 = \\{3, 3.5, 4, 7\\}$\n\n#### **Compiti:**\n1. **Calcolo dei parametri delle popolazioni:**  \n   - Determinare la media e la varianza di entrambe le popolazioni.\n   \n2. **Estrazione di campioni:**  \n   - Estrarre tutti i possibili campioni di ampiezza $n = 2$ con ripetizione da entrambe le popolazioni.\n   - Calcolare la media campionaria di ciascun campione.\n\n3. **Distribuzione campionaria della differenza tra le medie:**  \n   - Calcolare la differenza tra le medie di tutti i possibili campioni ottenuti dalle due popolazioni.\n   - Rappresentare graficamente la distribuzione della differenza tra le medie.\n\n4. **Probabilità della differenza tra le medie:**  \n   - Calcolare la probabilità che la differenza tra le medie campionarie sia maggiore di 1.\n\n**Esercizio 2: Simulazione con Popolazioni di Grande Dimensione**\n\nConsideriamo ora due popolazioni più grandi, distribuite normalmente:\n\n- **Popolazione 1:** $X_1 \\sim N(10, 4^2)$ (media = 10, deviazione standard = 4)\n- **Popolazione 2:** $X_2 \\sim N(8, 3^2)$ (media = 8, deviazione standard = 3)\n\n**Compiti:**\n\n1. **Generazione delle popolazioni:**  \n   - Creare due popolazioni casuali di 10.000 osservazioni ciascuna, distribuite normalmente.\n\n2. **Estrazione di campioni:**  \n   - Estrarre 10.000 campioni casuali di ampiezza $n_1 = 15$ dalla prima popolazione e $n_2 = 20$ dalla seconda popolazione.\n   - Calcolare la media di ciascun campione.\n\n3. **Distribuzione campionaria della differenza tra le medie:**  \n   - Calcolare la differenza tra le medie campionarie ottenute dalle due popolazioni.\n   - Rappresentare graficamente la distribuzione della differenza tra le medie.\n\n4. **Probabilità della differenza tra le medie:**  \n   - Calcolare la probabilità che la differenza tra le medie campionarie sia maggiore di 2 in due modi:\n     - **Metodo empirico:** utilizzando la distribuzione ottenuta nella simulazione.\n     - **Metodo teorico:** approssimando la distribuzione con una normale e calcolando la probabilità con la formula teorica della varianza della differenza tra le medie.\n\n**Domande di Discussione**\n\n1. Come cambia la forma della distribuzione campionaria al variare delle dimensioni dei campioni $n_1$ e $n_2$?\n2. La probabilità stimata tramite simulazione coincide con quella calcolata utilizzando l’approssimazione normale? Perché?\n3. Cosa accadrebbe alla distribuzione campionaria se le popolazioni originali non fossero normali? Quale sarebbe il ruolo del Teorema del Limite Centrale?\n:::\n\n::: {.callout-tip title=\"Soluzioni 2\" collapse=\"true\"}\n**Esercizio 1: Simulazione con Popolazioni di Piccola Dimensione**\n\nSupponiamo di avere due popolazioni finite definite come segue:\n\n- Popolazione 1: $x_1 = \\{2, 4.5, 5, 6\\}$\n- Popolazione 2: $x_2 = \\{3, 3.5, 4, 7\\}$\n\n1. Calcola le medie e le varianze delle due popolazioni:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Popolazione 1\nx1 <- c(2, 4.5, 5, 6)\nmean(x1)  # Media di x1\n#> [1] 4.38\nvar(x1)   # Varianza di x1\n#> [1] 2.9\n```\n:::\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Popolazione 2\nx2 <- c(3, 3.5, 4, 7)\nmean(x2)  # Media di x2\n#> [1] 4.38\nvar(x2)   # Varianza di x2\n#> [1] 3.23\n```\n:::\n\n\n2. Estrai tutti i possibili campioni di ampiezza $n = 2$ da entrambe le popolazioni:\n   \n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Tutti i campioni di ampiezza 2\nsamples1 <- expand.grid(x1, x1)\nsamples2 <- expand.grid(x2, x2)\n\n# Medie campionarie\nsample_means1 <- rowMeans(samples1)\nsample_means2 <- rowMeans(samples2)\n```\n:::\n\n\n3. Calcola la differenza tra le medie campionarie di ciascuna combinazione di campioni:\n   \n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Differenze tra le medie campionarie\nsample_diff <- as.vector(outer(sample_means1, sample_means2, \"-\"))\n```\n:::\n\n\n4. Visualizza la distribuzione campionaria della differenza tra le medie:\n   \n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Istogramma della differenza tra medie campionarie\n\n# Creazione del data frame per ggplot2\ndf_diff <- data.frame(sample_diff = sample_diff)\n\n# Visualizzazione con ggplot2\nggplot(df_diff, aes(x = sample_diff)) +\n  geom_histogram(aes(y = after_stat(density)), bins = 10, fill = \"blue\", alpha = 0.6) +\n  geom_density(color = \"red\", size = 1) +\n  labs(\n    title = \"Distribuzione campionaria della differenza tra medie\",\n    x = \"Differenza campionaria\",\n    y = \"Densità\"\n  )\n```\n\n::: {.cell-output-display}\n![](02_stime_parametri_files/figure-html/unnamed-chunk-36-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\n5. Calcola la probabilità che la differenza campionaria sia maggiore di 1:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Probabilità esatta dalla distribuzione campionaria\nexact_probability <- mean(sample_diff > 1)\nexact_probability\n#> [1] 0.266\n```\n:::\n\n\n**Esercizio 2: Simulazione con Popolazioni di Grande Dimensione**\n\nOra considera due popolazioni più grandi con distribuzioni normali:\n\n- Popolazione 1: $X_1 \\sim N(10, 4^2)$\n- Popolazione 2: $X_2 \\sim N(8, 3^2)$\n\n1. Genera due popolazioni casuali:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(123)\npop1 <- rnorm(10000, mean = 10, sd = 4)\npop2 <- rnorm(10000, mean = 8, sd = 3)\n```\n:::\n\n\n2. Estrai 10.000 campioni casuali di ampiezza $n_1 = 15$ e $n_2 = 20$ rispettivamente:\n   \n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Estrazione di campioni e calcolo delle medie\nsample_means1 <- replicate(10000, mean(sample(pop1, size = 15)))\nsample_means2 <- replicate(10000, mean(sample(pop2, size = 20)))\n```\n:::\n\n\n3. Calcola la differenza tra le medie campionarie:\n   \n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Differenze tra medie campionarie\nsample_diff_large <- sample_means1 - sample_means2\n```\n:::\n\n\n4. Visualizza la distribuzione campionaria della differenza tra le medie:\n   \n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Istogramma della distribuzione campionaria\n\n# Creazione del data frame per ggplot2\ndf_diff_large <- data.frame(sample_diff = sample_diff_large)\n\n# Visualizzazione con ggplot2\nggplot(df_diff_large, aes(x = sample_diff)) +\n  geom_histogram(aes(y = after_stat(density)), bins = 50, fill = \"blue\", alpha = 0.6) +\n  geom_density(color = \"red\", size = 1) +\n  labs(\n    title = \"Distribuzione campionaria della differenza tra medie (n1 = 15, n2 = 20)\",\n    x = \"Differenza campionaria\",\n    y = \"Densità\"\n  )\n```\n\n::: {.cell-output-display}\n![](02_stime_parametri_files/figure-html/unnamed-chunk-41-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\n5. Calcola la probabilità che la differenza campionaria sia maggiore di 2 utilizzando:\n\n   - la simulazione;\n   - l'approssimazione normale.\n   \n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Probabilità esatta\nexact_probability_large <- mean(sample_diff_large > 2)\nexact_probability_large\n#> [1] 0.505\n```\n:::\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Approssimazione normale\nmu_diff <- 10 - 8  # Differenza tra le medie delle popolazioni\nsigma_diff <- sqrt(4^2 / 15 + 3^2 / 20)  # Deviazione standard della differenza\napprox_probability_large <- 1 - pnorm(2, mean = mu_diff, sd = sigma_diff)\napprox_probability_large\n#> [1] 0.5\n```\n:::\n\n\n**Domande di Discussione.**\n\n1. Come cambia la forma della distribuzione campionaria al variare delle dimensioni campionarie $n_1$ e $n_2$?\n2. La probabilità calcolata tramite simulazione coincide con quella calcolata tramite approssimazione normale? Perché?\n3. Cosa accadrebbe alla distribuzione campionaria se le popolazioni originali non fossero normali?\n:::\n\n\n::: {.callout-important title=\"Problemi 3\" collapse=\"true\"}\n**Problema: Distribuzione Campionaria di una Proporzione**\n\nL'obiettivo di questo esercizio è esplorare la distribuzione campionaria di una proporzione campionaria $\\hat{p}$ e verificare l'applicabilità dell'approssimazione normale per grandi dimensioni campionarie, come previsto dal Teorema del Limite Centrale.\n\n**Situazione**\n\nSupponiamo di avere una popolazione infinita in cui ciascun individuo può appartenere a una di due categorie: \"successo\" (codificato come 1) o \"insuccesso\" (codificato come 0). La probabilità di successo nella popolazione è data da $p = 0.6$.\n\n**Compiti**\n\n1. **Definizione della popolazione e dei parametri:**  \n   - La popolazione ha una proporzione di successi pari a $p = 0.6$.\n   - La deviazione standard teorica della distribuzione campionaria della proporzione è calcolata come:  \n     $$\n     \\text{SD}(\\hat{p}) = \\sqrt{\\frac{p(1 - p)}{n}}\n     $$\n   - Si fissi una dimensione campionaria pari a $n = 100$.\n\n2. **Simulazione di campioni:**  \n   - Generare 10.000 campioni casuali di ampiezza $n = 100$.\n   - Per ogni campione, calcolare la proporzione campionaria $\\hat{p}$, cioè la frazione di successi nel campione.\n\n3. **Distribuzione campionaria delle proporzioni:**  \n   - Rappresentare graficamente la distribuzione delle proporzioni campionarie con un istogramma.\n   - Sovrapporre la distribuzione normale teorica $N(p, \\text{SD}(\\hat{p}))$ per confrontare l'andamento empirico con quello teorico.\n\n4. **Analisi della distribuzione:**  \n   - Valutare se la distribuzione campionaria ottenuta rispetta l’approssimazione normale.\n   - Riflettere su come la dimensione del campione e il valore di $p$ influenzano questa approssimazione.\n\n**Domande di Discussione**\n\n1. La distribuzione campionaria delle proporzioni $\\hat{p}$ sembra approssimarsi a una distribuzione normale? Perché?\n2. Cosa accadrebbe se la dimensione del campione fosse più piccola, ad esempio $n = 30$?\n3. Se la probabilità di successo $p$ fosse molto vicina a 0 o 1, l'approssimazione normale sarebbe ancora valida? Perché?\n:::\n\n::: {.callout-tip title=\"Soluzioni 3\" collapse=\"true\"}\nLa distribuzione campionaria di una proporzione descrive come la proporzione campionaria ($\\hat{p}$) varia tra campioni di una popolazione. Per campioni grandi, il Teorema del Limite Centrale garantisce che la distribuzione campionaria di $\\hat{p}$ può essere approssimata con una distribuzione normale.\n\n1. **Supponiamo una popolazione infinita** in cui la probabilità di successo ($p$) è $p = 0.6$. Scegli una dimensione campionaria $n = 100$.\n2. Simula 10.000 campioni casuali di ampiezza $n$ e calcola le proporzioni campionarie ($\\hat{p}$).\n3. Confronta l'istogramma delle proporzioni campionarie con la distribuzione normale teorica approssimativa.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Parametri della popolazione\np <- 0.6  # Probabilità di successo nella popolazione\nn <- 100  # Dimensione del campione\n\n# Simulazione di 10.000 campioni\nset.seed(123)\nsample_props <- replicate(10000, mean(rbinom(n, size = 1, prob = p)))\n\n# Creazione di un data frame per ggplot2\ndf_props <- data.frame(sample_props = sample_props)\n\n# Parametri della distribuzione normale teorica\nmean_theoretical <- p\nsd_theoretical <- sqrt(p * (1 - p) / n)\n\n# Visualizzazione con ggplot2\nggplot(df_props, aes(x = sample_props)) +\n  geom_histogram(aes(y = after_stat(density)), bins = 40, fill = \"blue\", alpha = 0.6) +\n  stat_function(\n    fun = dnorm,\n    args = list(mean = mean_theoretical, sd = sd_theoretical),\n    color = \"red\",\n    size = 1\n  ) +\n  labs(\n    title = \"Distribuzione campionaria di una proporzione (n = 100)\",\n    x = \"Proporzione campionaria (\\u0302p)\",\n    y = \"Densità\"\n  ) \n```\n\n::: {.cell-output-display}\n![](02_stime_parametri_files/figure-html/unnamed-chunk-44-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\n1. **Popolazione e parametri**:\n   - La popolazione è definita da $p = 0.6$.\n   - La deviazione standard teorica della distribuzione campionaria è calcolata come $\\text{SD}(\\hat{p}) = \\sqrt{\\frac{p(1-p)}{n}}$.\n\n2. **Simulazione**:\n   - Per ogni campione, i successi ($0$ o $1$) sono generati con `rbinom`, e la proporzione campionaria $\\hat{p}$ è calcolata come media.\n\n3. **Grafico**:\n   - L'istogramma delle proporzioni campionarie è sovrapposto alla curva normale teorica ($N(p, \\text{SD}(\\hat{p}))$).\n\n**Domande di Discussione.**\n\n1. La distribuzione campionaria di $\\hat{p}$ sembra approssimarsi a una distribuzione normale? Perché?\n2. Cosa accadrebbe se $n$ fosse più piccolo (es. $n = 30$)?\n3. Se $p$ fosse più vicino a $0$ o $1$, l'approssimazione normale sarebbe ancora valida? Spiega. \n:::\n\n::: {.callout-important title=\"Problemi 4\" collapse=\"true\"}\n**Problema: Distribuzione Campionaria della Differenza tra Due Proporzioni**\n\nL'obiettivo di questo esercizio è esplorare la distribuzione campionaria della differenza tra due proporzioni campionarie, $\\hat{p}_1 - \\hat{p}_2$, ottenute da due campioni indipendenti estratti da popolazioni diverse. Per campioni sufficientemente grandi, il Teorema del Limite Centrale garantisce che la distribuzione di $\\hat{p}_1 - \\hat{p}_2$ può essere approssimata con una distribuzione normale.\n\n**Situazione**\n\nAbbiamo due popolazioni con proporzioni di successo diverse:\n\n- **Popolazione 1** ha una proporzione di successo $p_1 = 0.6$.\n- **Popolazione 2** ha una proporzione di successo $p_2 = 0.4$.\n\nVogliamo studiare il comportamento della distribuzione campionaria della differenza tra le proporzioni campionarie quando preleviamo campioni indipendenti da ciascuna popolazione.\n\n**Compiti**\n\n1. **Definizione delle popolazioni e dei parametri:**  \n   - La proporzione di successi nelle due popolazioni è $p_1 = 0.6$ e $p_2 = 0.4$.\n   - Entrambi i campioni hanno dimensione $n_1 = n_2 = 150$.\n   - La deviazione standard teorica della distribuzione campionaria della differenza tra proporzioni è calcolata come:\n     $$\n     \\text{SD}(\\hat{p}_1 - \\hat{p}_2) = \\sqrt{\\frac{p_1 (1-p_1)}{n_1} + \\frac{p_2 (1-p_2)}{n_2}}\n     $$\n\n2. **Simulazione di campioni:**  \n   - Generare 10.000 campioni indipendenti di ampiezza $n_1 = 150$ dalla prima popolazione e $n_2 = 150$ dalla seconda popolazione.\n   - Calcolare la proporzione campionaria $\\hat{p}_1$ e $\\hat{p}_2$ per ciascun campione.\n   - Calcolare la differenza tra le proporzioni campionarie.\n\n3. **Distribuzione campionaria della differenza tra le proporzioni:**  \n   - Rappresentare graficamente la distribuzione di $\\hat{p}_1 - \\hat{p}_2$ con un istogramma.\n   - Sovrapporre la distribuzione normale teorica $N(p_1 - p_2, \\text{SD}(\\hat{p}_1 - \\hat{p}_2))$ per confrontare l'andamento empirico con quello teorico.\n\n4. **Calcolo della probabilità che la differenza tra proporzioni sia maggiore di un valore specifico:**  \n   - **Metodo empirico:** calcolare la proporzione di campioni in cui $\\hat{p}_1 - \\hat{p}_2 > 0.1$.\n   - **Metodo teorico:** utilizzare la distribuzione normale approssimata per stimare la probabilità che $\\hat{p}_1 - \\hat{p}_2 > 0.1$.\n\n**Domande di Discussione**\n\n1. L'approssimazione normale è valida in questo caso? Perché?\n2. Come cambierebbe la distribuzione campionaria se la dimensione del campione $n_1$ o $n_2$ fosse più piccola?\n3. Se i valori di $p_1$ o $p_2$ fossero più vicini a 0 o 1, come cambierebbe la probabilità calcolata e l'accuratezza dell'approssimazione normale?\n:::\n\n::: {.callout-tip title=\"Soluzioni 4\" collapse=\"true\"}\nLa distribuzione campionaria della differenza tra due proporzioni ($\\hat{p}_1 - \\hat{p}_2$) descrive come la differenza tra le proporzioni campionarie varia tra due campioni indipendenti estratti da due popolazioni.\n\nPer campioni grandi, il Teorema del Limite Centrale garantisce che $\\hat{p}_1 - \\hat{p}_2$ segue approssimativamente una distribuzione normale con media e varianza calcolate dalle proporzioni della popolazione.\n\nObiettivo:\n\n1. Simulare due popolazioni con proporzioni $p_1$ e $p_2$.\n2. Estrarre campioni indipendenti da ciascuna popolazione.\n3. Calcolare la distribuzione campionaria di $\\hat{p}_1 - \\hat{p}_2$.\n4. Calcolare la probabilità che la differenza campionaria sia maggiore di un valore specifico (es. $0.1$).\n\nParametri del Problema:\n\n- Popolazione 1: $p_1 = 0.6$\n- Popolazione 2: $p_2 = 0.4$\n- Dimensione campionaria: $n_1 = n_2 = 150$\n- Valore specifico: $0.1$\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Parametri delle due popolazioni\np1 <- 0.6  # Proporzione di successi nella Popolazione 1\np2 <- 0.4  # Proporzione di successi nella Popolazione 2\nn1 <- 150  # Dimensione campionaria per la Popolazione 1\nn2 <- 150  # Dimensione campionaria per la Popolazione 2\n\n# Simulazione di 10.000 campioni indipendenti\nset.seed(123)\nsample_p1 <- replicate(10000, mean(rbinom(n1, size = 1, prob = p1)))\nsample_p2 <- replicate(10000, mean(rbinom(n2, size = 1, prob = p2)))\n\n# Calcolo della differenza tra proporzioni campionarie\nsample_diff <- sample_p1 - sample_p2\n\n# Parametri teorici della distribuzione normale approssimata\nmean_diff <- p1 - p2\nsd_diff <- sqrt((p1 * (1 - p1) / n1) + (p2 * (1 - p2) / n2))\n\n# Creazione del data frame per ggplot2\ndf_diff <- data.frame(sample_diff = sample_diff)\n\n# Visualizzazione con ggplot2\nggplot(df_diff, aes(x = sample_diff)) +\n  geom_histogram(\n    aes(y = after_stat(density)), bins = 40, fill = \"blue\", alpha = 0.6\n  ) +\n  stat_function(\n    fun = dnorm,\n    args = list(mean = mean_diff, sd = sd_diff),\n    color = \"red\",\n    size = 1\n  ) +\n  labs(\n    title = \"Distribuzione campionaria della differenza tra proporzioni\",\n    x = \"Differenza campionaria (p1 - p2)\",\n    y = \"Densità\"\n  )\n```\n\n::: {.cell-output-display}\n![](02_stime_parametri_files/figure-html/unnamed-chunk-45-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Calcolo della probabilità che la differenza sia maggiore di 0.1\nexact_probability <- mean(sample_diff > 0.1)\nexact_probability\n#> [1] 0.96\n```\n:::\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Approssimazione tramite distribuzione normale\napprox_probability <- 1 - pnorm(0.1, mean = mean_diff, sd = sd_diff)\napprox_probability\n#> [1] 0.961\n```\n:::\n\n\n1. **Parametri delle popolazioni**:\n   - Due popolazioni con proporzioni $p_1 = 0.6$ e $p_2 = 0.4$.\n   - Dimensioni campionarie $n_1 = n_2 = 150$.\n\n2. **Simulazione**:\n   - Per ogni campione, i successi ($0$ o $1$) sono generati con `rbinom`.\n   - Calcoliamo le proporzioni campionarie e la differenza tra di esse.\n\n3. **Distribuzione teorica**:\n   - Media teorica: $p_1 - p_2$.\n   - Deviazione standard teorica: $\\sqrt{\\frac{p_1 (1-p_1)}{n_1} + \\frac{p_2 (1-p_2)}{n_2}}$.\n\n4. **Visualizzazione**:\n   - Un istogramma di $\\hat{p}_1 - \\hat{p}_2$ sovrapposto alla curva della distribuzione normale teorica.\n\n5. **Calcolo della probabilità**:\n   - Probabilità esatta dalla simulazione: proporzione di $\\hat{p}_1 - \\hat{p}_2 > 0.1$.\n   - Probabilità approssimata dalla distribuzione normale.\n\n**Domande di Discussione.**\n\n1. L'approssimazione normale è valida in questo caso? Perché?\n2. Come cambierebbe la distribuzione campionaria se $n_1$ o $n_2$ fossero più piccoli?\n3. Come influenzerebbe la probabilità calcolata un valore $p_1$ o $p_2$ più vicino a $0$ o $1$? \n:::\n\n## Informazioni sull'Ambiente di Sviluppo {.unnumbered}\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsessionInfo()\n#> R version 4.5.1 (2025-06-13)\n#> Platform: aarch64-apple-darwin20\n#> Running under: macOS Sequoia 15.6.1\n#> \n#> Matrix products: default\n#> BLAS:   /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRblas.0.dylib \n#> LAPACK: /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.1\n#> \n#> locale:\n#> [1] C/UTF-8/C/C/C/C\n#> \n#> time zone: Europe/Rome\n#> tzcode source: internal\n#> \n#> attached base packages:\n#> [1] stats     graphics  grDevices utils     datasets  methods   base     \n#> \n#> other attached packages:\n#>  [1] pillar_1.11.0         tinytable_0.13.0      patchwork_1.3.2      \n#>  [4] ggdist_3.3.3          tidybayes_3.0.7       bayesplot_1.14.0     \n#>  [7] ggplot2_3.5.2         reliabilitydiag_0.2.1 priorsense_1.1.1     \n#> [10] posterior_1.6.1       loo_2.8.0             rstan_2.32.7         \n#> [13] StanHeaders_2.32.10   brms_2.22.0           Rcpp_1.1.0           \n#> [16] sessioninfo_1.2.3     conflicted_1.2.0      janitor_2.2.1        \n#> [19] matrixStats_1.5.0     modelr_0.1.11         tibble_3.3.0         \n#> [22] dplyr_1.1.4           tidyr_1.3.1           rio_1.2.3            \n#> [25] here_1.0.1           \n#> \n#> loaded via a namespace (and not attached):\n#>  [1] svUnit_1.0.8          tidyselect_1.2.1      farver_2.1.2         \n#>  [4] fastmap_1.2.0         TH.data_1.1-4         tensorA_0.36.2.1     \n#>  [7] digest_0.6.37         timechange_0.3.0      estimability_1.5.1   \n#> [10] lifecycle_1.0.4       survival_3.8-3        magrittr_2.0.3       \n#> [13] compiler_4.5.1        rlang_1.1.6           tools_4.5.1          \n#> [16] knitr_1.50            labeling_0.4.3        bridgesampling_1.1-2 \n#> [19] htmlwidgets_1.6.4     curl_7.0.0            pkgbuild_1.4.8       \n#> [22] RColorBrewer_1.1-3    abind_1.4-8           multcomp_1.4-28      \n#> [25] withr_3.0.2           purrr_1.1.0           grid_4.5.1           \n#> [28] stats4_4.5.1          colorspace_2.1-1      xtable_1.8-4         \n#> [31] inline_0.3.21         emmeans_1.11.2-8      scales_1.4.0         \n#> [34] MASS_7.3-65           cli_3.6.5             mvtnorm_1.3-3        \n#> [37] rmarkdown_2.29        ragg_1.5.0            generics_0.1.4       \n#> [40] RcppParallel_5.1.11-1 cachem_1.1.0          stringr_1.5.1        \n#> [43] splines_4.5.1         parallel_4.5.1        vctrs_0.6.5          \n#> [46] V8_7.0.0              Matrix_1.7-4          sandwich_3.1-1       \n#> [49] jsonlite_2.0.0        arrayhelpers_1.1-0    systemfonts_1.2.3    \n#> [52] glue_1.8.0            codetools_0.2-20      distributional_0.5.0 \n#> [55] lubridate_1.9.4       stringi_1.8.7         gtable_0.3.6         \n#> [58] QuickJSR_1.8.0        htmltools_0.5.8.1     Brobdingnag_1.2-9    \n#> [61] R6_2.6.1              textshaping_1.0.3     rprojroot_2.1.1      \n#> [64] evaluate_1.0.5        lattice_0.22-7        backports_1.5.0      \n#> [67] memoise_2.0.1         broom_1.0.9           snakecase_0.11.1     \n#> [70] rstantools_2.5.0      coda_0.19-4.1         gridExtra_2.3        \n#> [73] nlme_3.1-168          checkmate_2.3.3       xfun_0.53            \n#> [76] zoo_1.8-14            pkgconfig_2.0.3\n```\n:::\n\n\n",
    "supporting": [
      "02_stime_parametri_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}